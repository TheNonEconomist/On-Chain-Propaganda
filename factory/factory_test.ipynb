{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'emotion': 'angry', 'situation': 'missed_deadline', 'text': 'Why do Devs keep missing deadline!?!'}, {'emotion': 'frustration', 'situation': 'missed_deadline', 'text': 'Devs keep missing deadline?'}, {'emotion': 'frustration', 'situation': 'community_hopium', 'text': 'Smelling Hopium In the Air?'}, {'emotion': 'frustration', 'situation': 'productivity_encouragement', 'text': 'OG Team too late?'}, {'emotion': 'Happiness', 'situation': 'new_app_launch', 'text': 'Make sure it actually works?'}, {'emotion': 'Desperation', 'situation': 'original_team_MIA', 'text': \"Te isn't showing up?\"}]\n"
     ]
    }
   ],
   "source": [
    "CURRENT_PATH = os.getenv('CURRENT_PATH')\n",
    "with open(CURRENT_PATH + \"/factory/context/community_mgmt/materials.json\", \"r\") as f:\n",
    "        materials = json.load(f)\n",
    "        print(materials[\"top_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from factory import *\n",
    "from factory_widgets import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Backend Interface is \n",
    "# 1) Grab emotion and situation\n",
    "# 2) Search through materials\n",
    "# 3) Create appropriate combination\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion, situation = \"Happiness\", \"new_app_launch\"\n",
    "\n",
    "pic = emotion_and_situation_to_picture(emotion, situation, materials)\n",
    "tt = emotion_and_situation_to_top_text(emotion, situation, materials)\n",
    "bt = emotion_and_situation_to_bottom_text(emotion, situation, materials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_path = None\n",
    "for item in materials[\"pic_path\"]:\n",
    "    if item[\"emotion\"] == emotion and item[\"situation\"] == situation:\n",
    "        pic_path = item[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Make sure it actually works?', 'App finally launched.')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt, bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512, 3)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h, w, d = pic.shape\n",
    "h, w, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(243, 138, 84)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "color = grab_background_color(pic, \"corners_and_edges\")\n",
    "color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(255, 255, 255)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r, g, b = color\n",
    "font_color = (int(255 - r), int(255 - g), int(255 - b))\n",
    "font_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.uint8'> <class 'int'>\n",
      "<class 'numpy.uint8'> <class 'int'>\n",
      "<class 'numpy.uint8'> <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "for i,j in zip(color, font_color):\n",
    "    print(type(i), type(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pad = create_pad_and_text_wif_font_scaling(\n",
    "    color, h//3, w, d, \n",
    "    tt, (w//2, h//6), \"center\",\n",
    "    font=cv2.FONT_HERSHEY_COMPLEX\n",
    ")\n",
    "bottom_pad = create_pad_and_text_wif_font_scaling(\n",
    "    color, h//3, w, d, \n",
    "    bt, (w//2, h//6), \"center\",\n",
    "    font=cv2.FONT_HERSHEY_COMPLEX\n",
    ")\n",
    "# bottom_pad = create_pad_and_text(\n",
    "#     color, h//3, w, d, \n",
    "#     bt, (w//2, h//6), \"center\",\n",
    "#     font=cv2.FONT_HERSHEY_COMPLEX, font_scale=0.55, font_color=font_color, font_thickness=1\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_background_color(image, new_background_color):\n",
    "    mask = np.zeros(image.shape[:2], np.uint8)\n",
    "    bgdModel = np.zeros((1, 65), np.float64)\n",
    "    fgdModel = np.zeros((1, 65), np.float64)\n",
    "    \n",
    "    height, width = image.shape[:2]\n",
    "    rect = (10, 10, width-10, height-10)\n",
    "    \n",
    "    cv2.grabCut(image, mask, rect, bgdModel, fgdModel, 5, cv2.GC_INIT_WITH_RECT)\n",
    "    mask2 = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')\n",
    "    foreground = image * mask2[:, :, np.newaxis]\n",
    "\n",
    "    background = np.full(image.shape, new_background_color, dtype=np.uint8)\n",
    "    \n",
    "    # Combine the foreground with the new background\n",
    "    inverse_mask = 1 - mask2\n",
    "    new_background = background * inverse_mask[:, :, np.newaxis]\n",
    "    result = cv2.add(foreground, new_background)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /Users/keonshikkim/VisualStudio/On-Chain-Propaganda/programamble_ip/lib/python3.10/site-packages (3.3.3)\n",
      "Requirement already satisfied: numpy in /Users/keonshikkim/VisualStudio/On-Chain-Propaganda/programamble_ip/lib/python3.10/site-packages (from keras) (1.26.4)\n",
      "Requirement already satisfied: rich in /Users/keonshikkim/VisualStudio/On-Chain-Propaganda/programamble_ip/lib/python3.10/site-packages (from keras) (13.7.1)\n",
      "Requirement already satisfied: ml-dtypes in /Users/keonshikkim/VisualStudio/On-Chain-Propaganda/programamble_ip/lib/python3.10/site-packages (from keras) (0.3.2)\n",
      "Requirement already satisfied: namex in /Users/keonshikkim/VisualStudio/On-Chain-Propaganda/programamble_ip/lib/python3.10/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: absl-py in /Users/keonshikkim/VisualStudio/On-Chain-Propaganda/programamble_ip/lib/python3.10/site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: optree in /Users/keonshikkim/VisualStudio/On-Chain-Propaganda/programamble_ip/lib/python3.10/site-packages (from keras) (0.11.0)\n",
      "Requirement already satisfied: h5py in /Users/keonshikkim/VisualStudio/On-Chain-Propaganda/programamble_ip/lib/python3.10/site-packages (from keras) (3.11.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/keonshikkim/VisualStudio/On-Chain-Propaganda/programamble_ip/lib/python3.10/site-packages (from optree->keras) (4.12.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/keonshikkim/VisualStudio/On-Chain-Propaganda/programamble_ip/lib/python3.10/site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/keonshikkim/VisualStudio/On-Chain-Propaganda/programamble_ip/lib/python3.10/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/keonshikkim/VisualStudio/On-Chain-Propaganda/programamble_ip/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.api.utils' has no attribute 'generic_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/keonshikkim/VisualStudio/On-Chain-Propaganda/factory/factory_test.ipynb Cell 15\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/keonshikkim/VisualStudio/On-Chain-Propaganda/factory/factory_test.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/keonshikkim/VisualStudio/On-Chain-Propaganda/factory/factory_test.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/keonshikkim/VisualStudio/On-Chain-Propaganda/factory/factory_test.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msegmentation_models\u001b[39;00m \u001b[39mimport\u001b[39;00m Unet\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/keonshikkim/VisualStudio/On-Chain-Propaganda/factory/factory_test.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msegmentation_models\u001b[39;00m \u001b[39mimport\u001b[39;00m backbones\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/keonshikkim/VisualStudio/On-Chain-Propaganda/factory/factory_test.ipynb#X26sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_image\u001b[39m(image_path):\n",
      "File \u001b[0;32m~/VisualStudio/On-Chain-Propaganda/programamble_ip/lib/python3.10/site-packages/segmentation_models/__init__.py:98\u001b[0m\n\u001b[1;32m     96\u001b[0m _framework \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mSM_FRAMEWORK\u001b[39m\u001b[39m'\u001b[39m, _DEFAULT_KERAS_FRAMEWORK)\n\u001b[1;32m     97\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m     set_framework(_framework)\n\u001b[1;32m     99\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     other \u001b[39m=\u001b[39m _TF_KERAS_FRAMEWORK_NAME \u001b[39mif\u001b[39;00m _framework \u001b[39m==\u001b[39m _KERAS_FRAMEWORK_NAME \u001b[39melse\u001b[39;00m _KERAS_FRAMEWORK_NAME\n",
      "File \u001b[0;32m~/VisualStudio/On-Chain-Propaganda/programamble_ip/lib/python3.10/site-packages/segmentation_models/__init__.py:68\u001b[0m, in \u001b[0;36mset_framework\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39m==\u001b[39m _KERAS_FRAMEWORK_NAME:\n\u001b[1;32m     67\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mkeras\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mefficientnet\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m  \u001b[39m# init custom objects\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[39melif\u001b[39;00m name \u001b[39m==\u001b[39m _TF_KERAS_FRAMEWORK_NAME:\n\u001b[1;32m     70\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mimport\u001b[39;00m keras\n",
      "File \u001b[0;32m~/VisualStudio/On-Chain-Propaganda/programamble_ip/lib/python3.10/site-packages/efficientnet/keras.py:17\u001b[0m\n\u001b[1;32m     13\u001b[0m EfficientNetB7 \u001b[39m=\u001b[39m inject_keras_modules(model\u001b[39m.\u001b[39mEfficientNetB7)\n\u001b[1;32m     15\u001b[0m preprocess_input \u001b[39m=\u001b[39m inject_keras_modules(model\u001b[39m.\u001b[39mpreprocess_input)\n\u001b[0;32m---> 17\u001b[0m init_keras_custom_objects()\n",
      "File \u001b[0;32m~/VisualStudio/On-Chain-Propaganda/programamble_ip/lib/python3.10/site-packages/efficientnet/__init__.py:71\u001b[0m, in \u001b[0;36minit_keras_custom_objects\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m model\n\u001b[1;32m     66\u001b[0m custom_objects \u001b[39m=\u001b[39m {\n\u001b[1;32m     67\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mswish\u001b[39m\u001b[39m'\u001b[39m: inject_keras_modules(model\u001b[39m.\u001b[39mget_swish)(),\n\u001b[1;32m     68\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mFixedDropout\u001b[39m\u001b[39m'\u001b[39m: inject_keras_modules(model\u001b[39m.\u001b[39mget_dropout)()\n\u001b[1;32m     69\u001b[0m }\n\u001b[0;32m---> 71\u001b[0m keras\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mgeneric_utils\u001b[39m.\u001b[39mget_custom_objects()\u001b[39m.\u001b[39mupdate(custom_objects)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'keras.api.utils' has no attribute 'generic_utils'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from segmentation_models import Unet\n",
    "from segmentation_models import backbones\n",
    "\n",
    "def load_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "def preprocess_image(image, backbone_name='resnet34'):\n",
    "    preprocessing_fn = backbones.get_preprocessing(backbone_name)\n",
    "    image = cv2.resize(image, (256, 256))\n",
    "    image = preprocessing_fn(image)\n",
    "    return image\n",
    "\n",
    "def segment_image(image, model):\n",
    "    preprocessed_image = preprocess_image(image)\n",
    "    preprocessed_image = np.expand_dims(preprocessed_image, axis=0)\n",
    "    mask = model.predict(preprocessed_image)[0]\n",
    "    mask = (mask > 0.5).astype(np.uint8)\n",
    "    return mask\n",
    "\n",
    "def change_background(image, mask, new_background_color=(0, 255, 0)):\n",
    "    mask = cv2.resize(mask, (image.shape[1], image.shape[0]))\n",
    "    foreground = image * np.expand_dims(mask, axis=-1)\n",
    "    background = np.full_like(image, new_background_color)\n",
    "    background = background * np.expand_dims(1 - mask, axis=-1)\n",
    "    result = foreground + background\n",
    "    return result\n",
    "\n",
    "# Load a pre-trained U-Net model\n",
    "model = Unet('resnet34', encoder_weights='imagenet', input_shape=(256, 256, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = change_background_color(pic, (94, 218, 250))\n",
    "cv2.imwrite(\"background_change_test.png\", new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test neural net based background sourcing\n",
    "# Usage example\n",
    "mask = segment_image(pic, model)\n",
    "result = change_background(image, mask, new_background_color=(94, 218, 250))\n",
    "cv2.imwrite(\"background_change_neuralnet_test.png\", new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite(\"original.png\", pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pic = concatenate_images_inline([top_pad, pic, bottom_pad], horizontal=False)\n",
    "cv2.imwrite(\"test_propaganda.png\", new_pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npbc = change_background_color(pic, (94, 218, 250)) # bgr\n",
    "cv2.imwrite(\"test_background_color_change.png\", npbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in /Users/keonshikkim/VisualStudio/On-Chain-Propaganda/programamble_ip/lib/python3.10/site-packages (2.3.1)\n",
      "Requirement already satisfied: torchvision in /Users/keonshikkim/VisualStudio/On-Chain-Propaganda/programamble_ip/lib/python3.10/site-packages (0.18.1)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.3.1-cp310-cp310-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /Users/keonshikkim/VisualStudio/On-Chain-Propaganda/programamble_ip/lib/python3.10/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: filelock in /Users/keonshikkim/VisualStudio/On-Chain-Propaganda/programamble_ip/lib/python3.10/site-packages (from torch) (3.15.1)\n",
      "Requirement already satisfied: fsspec in /Users/keonshikkim/VisualStudio/On-Chain-Propaganda/programamble_ip/lib/python3.10/site-packages (from torch) (2024.6.0)\n",
      "Requirement already satisfied: sympy in /Users/keonshikkim/VisualStudio/On-Chain-Propaganda/programamble_ip/lib/python3.10/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: jinja2 in /Users/keonshikkim/VisualStudio/On-Chain-Propaganda/programamble_ip/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/keonshikkim/VisualStudio/On-Chain-Propaganda/programamble_ip/lib/python3.10/site-packages (from torch) (4.12.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/keonshikkim/VisualStudio/On-Chain-Propaganda/programamble_ip/lib/python3.10/site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: numpy in /Users/keonshikkim/VisualStudio/On-Chain-Propaganda/programamble_ip/lib/python3.10/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/keonshikkim/VisualStudio/On-Chain-Propaganda/programamble_ip/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /Users/keonshikkim/VisualStudio/On-Chain-Propaganda/programamble_ip/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "Installing collected packages: torchaudio\n",
      "Successfully installed torchaudio-2.3.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Pipeline cannot infer suitable model classes from briaai/RMBG-1.4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/keonshikkim/VisualStudio/On-Chain-Propaganda/factory/factory_test.ipynb Cell 22\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/keonshikkim/VisualStudio/On-Chain-Propaganda/factory/factory_test.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Use a pipeline as a high-level helper\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/keonshikkim/VisualStudio/On-Chain-Propaganda/factory/factory_test.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m pipeline\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/keonshikkim/VisualStudio/On-Chain-Propaganda/factory/factory_test.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m pipe \u001b[39m=\u001b[39m pipeline(\u001b[39m\"\u001b[39;49m\u001b[39mimage-segmentation\u001b[39;49m\u001b[39m\"\u001b[39;49m, model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mbriaai/RMBG-1.4\u001b[39;49m\u001b[39m\"\u001b[39;49m, trust_remote_code\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/VisualStudio/On-Chain-Propaganda/programamble_ip/lib/python3.10/site-packages/transformers/pipelines/__init__.py:906\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(model, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m framework \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    905\u001b[0m     model_classes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[0;32m--> 906\u001b[0m     framework, model \u001b[39m=\u001b[39m infer_framework_load_model(\n\u001b[1;32m    907\u001b[0m         model,\n\u001b[1;32m    908\u001b[0m         model_classes\u001b[39m=\u001b[39;49mmodel_classes,\n\u001b[1;32m    909\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[1;32m    910\u001b[0m         framework\u001b[39m=\u001b[39;49mframework,\n\u001b[1;32m    911\u001b[0m         task\u001b[39m=\u001b[39;49mtask,\n\u001b[1;32m    912\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs,\n\u001b[1;32m    913\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m    914\u001b[0m     )\n\u001b[1;32m    916\u001b[0m model_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\n\u001b[1;32m    917\u001b[0m hub_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m~/VisualStudio/On-Chain-Propaganda/programamble_ip/lib/python3.10/site-packages/transformers/pipelines/base.py:264\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     class_tuple \u001b[39m=\u001b[39m class_tuple \u001b[39m+\u001b[39m \u001b[39mtuple\u001b[39m(classes)\n\u001b[1;32m    263\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(class_tuple) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 264\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPipeline cannot infer suitable model classes from \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    266\u001b[0m all_traceback \u001b[39m=\u001b[39m {}\n\u001b[1;32m    267\u001b[0m \u001b[39mfor\u001b[39;00m model_class \u001b[39min\u001b[39;00m class_tuple:\n",
      "\u001b[0;31mValueError\u001b[0m: Pipeline cannot infer suitable model classes from briaai/RMBG-1.4"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"image-segmentation\", model=\"briaai/RMBG-1.4\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/keonshikkim/Desktop/ART PROJECT/Materials/moncat/monc_cool.webp'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pic_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "programamble_ip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
